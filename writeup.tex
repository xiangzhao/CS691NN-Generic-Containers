%% LyX 2.0.0rc3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[twocolumn,english,natbib]{sigplanconf}
\usepackage[T1]{fontenc}
\usepackage{array}
\usepackage{amsmath}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\newcommand{\noun}[1]{\textsc{#1}}
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

\makeatother

\usepackage{babel}
\begin{document}

\CopyrightYear{2011}


\title{Adaptivity in STL sequence data structures}


\authorinfo{Eli Gottlieb\and Xiang Zhao}{University of Massachusetts Amherst}{egottlie@student.umass.edu\\
xiang@cs.umass.edu}
\maketitle
\begin{abstract}
The C++ Standard Template Library provides a variety of sequence data
structures, each of which has different performance characteristics.
Performing the wrong operations upon a sequence \emph{en masse} can
result in major performance penalties. We have therefore created an
\emph{adaptive} sequence in C++ that changes its internal representation
to the optimal data structure for the operations being performed upon
it. We present the adaptive sequence's design and evaluate its performance
characteristics when tested against the original C++ STL.
\end{abstract}

\terms{Algorithms, Performance, Design}


\section{Introduction and Motivation}

The C++ Standard Template Library is one of the most widely used software
libraries in the world. For every \emph{concept} in the library, multiple
implementations are provided as concrete classes. The sequence concept
is provided in the vector, (doubly-linked) list, and deque (double-ended
queue) forms. Each of these has different performance characteristics
both in terms of asymptotic time complexity and memory locality, and
serious losses in performance have been observed\cite{Liu2009} due
to using the wrong data structure.

This situation runs against a core ideal in both systems and programming
languages: modularity. In a modular system, different components couple
loosely, via their interfaces alone, and components with identical
interfaces can thus be exchanged for one another transparently. The
Java programming language directly provides a means for component-based
programming in the form of interfaces\cite{Krone2003}. While the
C++ Standard Template Library does not include the public abstract
virtual super-classes for its data structures that would correspond
to Java interfaces, it does structure its different data-structure
and iterator implementations according to a fixed number of \emph{concepts},
specifications of what operations (and with what signatures) a template
parameter should support for use in a particular template. Concepts
have been drafted for inclusion in the C++ language standard\cite{ISO2009},
but have not been officially included yet. Despite their lack of official
inclusion, the STL provides implementations of a number of concepts
in its iterators and data structures.

Given different, loosely-coupled data structures and iterators implementing
the same concept or interface, programmers should be able to tune
the performance of their code by simply switching the particular implementation
under use and recompiling. Even programmers working on performance-critical
C++ applications should be able to simply ask for a {}``set'' or
a {}``sequence'' and not have to worry about the precise implementation
until the profiling stage of their development cycle, as can currently
be done using the Java Collections Framework. In fact, in the ideal
case the programmer should simply ask for a {}``set'' or {}``sequence''
and have a performance-tuning tool determine precisely what implementation
to use\emph{ for} them until such time as they feel the need to take
manual control.

Unfortunately, the STL does not document concepts common to all its
data structures implementing a given hypothetical abstract data-type,
only iterators, and it provides no interfaces in the Java sense at
all. We have attempted to remedy this situation, and to take a step
towards the ideal of self-tuning applications, by building an \emph{adaptive}
sequence template class for C++. Our adaptive sequence logs the method-calls
it receives and uses that history to \emph{adapt} its internal data
structure, changing to a list, vector, or deque for the optimal asymptotic
time complexity.


\subsection{Related Work}

Liu et al\cite{Liu2009} found that choosing the right STL data structure
can result in as much as a 17\% speed improvement, with many smaller
improvements being found. Their code analyzer did not allow for generic
treatment of sequences without regard to their implementation but
required that the code only employ the methods shared between all
the STL sequence classes.

A group of researchers set out to use machine-learning techniques
to decide what data structure should be used where, producing software
they called Brainy\cite{Jung2011}. Brainy used empirical time and
space measurements to determine the real relative performance of different
set implementations, and thus trained a neural network to analyze
code and determine what data structures to use. Brainy trained its
neural-network using randomly-generated applications built by its
\emph{application generator}, software that would have proved extremely
useful in benchmarking our own adaptive sequences. Brainy achieved
27\% and 33\% speed-ups on two different test machines.

Earlier work has also been done on adapting the data structures used
in a program to their workloads in the compiler. Bik and Wijshoff\cite{Bik1993}
wrote an optimizing compiler that would convert matrix data structures
into sparse representations for programmers. Even earlier on, Low
and Rovner\cite{Low1976} built an experimental system for optimizing
representations of lists, sets, and maps in an ALGOL-60-based programming
language. An implementation of adaptive data structures very similar
in goals to ours was later presented in Common Lisp by Wozniak, Daley,
and Watt\cite{Wozniak2007}, which they termed \emph{dynamic abstract
data types}.


\section{Design and API support}

Our adaptive sequence supports most of the combined method set of
\noun{std::list<T>}, \noun{std::vector<T>}, and \noun{std::deque<T>}.
It currently only lacks a small number of methods and the production
of random-access iterators rather than bidirectional iterators. It
can therefore be used as an imperfect but workable drop-in replacement
for many, if not most, uses of STL sequences. The STL does not specify
a particular concept or interface for any of its sequence data structures,
and this has prevented us from producing a genuine drop-in replacement.

The adaptive sequence logs the methods called upon it, and also keeps
track of the log's length. When the log reaches a certain length \emph{n},
the adaptive sequence calculates the amortized complexity of having
performed the entire logged set of operations with each possible internal
data structure, with an extra iteration over every item of the sequence
being included to account for the representation conversion itself.
It then creates a new instance of the possible internal data structure
(list, vector, or deque) that would have performed the logged operations
the fastest, copies the contents of the sequence into the new internal
data structure, replaces the internal data structure, and sets a dirty
flag to alert any active iterators to the change. Moving data elements
into the new internal sequence preserves their indices and ordering
within the sequence.

The internal STL vector, list, or deque itself is stored as a tagged
variant, on which tag switching is done to execute methods upon the
internal data structure. This means that rather than implementing
the internal sequences ourselves, we wrap around the highly-optimized
STL implementations in order to focus our primary efforts on the adaptation
machinery itself. When an STL data structure does not provide a method
we provide, such as the \noun{sort()} method, we copy out the elements
of our sequence into an alternative that \emph{does} support that
method, run the method on that temporary data structure, copy the
contents back into our original (if mutation could have occurred),
and return the relevant result -- logging all the necessary sequence
operations to account for the performance penalty at adaptation-time.

The drawback to our API implementation is that, since we only support
bidirectional iterators rather than random-access iterators, a number
of the STL \emph{algorithms} (a misnomer, as these are templated library
functions implementing common algorithms rather than just theoretical
algorithms) do not work on the adaptive sequence at this time. Each
iterator operation can also only count for a single constant-time
operation alone. Such operations are not logged because this would
result in logging N constant-time \noun{ACCESS\_ELEMENT} operations
for a full iteration over a sequence of N elements rather than logging
an \noun{ITERATE\_OVER} operation. This would throw off the accuracy
of our amortized-complexity calculations.


\section{Performance evaluation}

We evaluated the performance of our adaptive sequence in a number
of iterations, each one coming after applying several more optimizations
to our implementation of the adaptive sequence itself. To evaluate
our adaptive sequence, we applied fixed workloads of method calls
to each of the separate STL sequences supporting the relevant methods
and to our adaptive sequence, and recorded the time taken by each
data structure to perform the workload. All data elements for sequences
were C++ \noun{int}'s, being the most convenient and compact data
objects to create arbitrarily or at random.

We ran the first two iterations of our tests on Ubuntu 10.04.2 LTS
running on an Intel Core 2 Duo 2.53GHz, with 3072 kilobytes of cache,
2048 megabytes of RAM and 2730 megabytes of swap space. The code was
compiled with g++ (Ubuntu 4.4.3-4ubuntu5) 4.4.3.


\subsection{Iteration 1: naive implementation}

Our unoptimized code was unequivocally slow. We have included data
only for the test workload that finished inside of 3 minutes.

\begin{table*}[tbh]
\begin{tabular}{|>{\centering}p{0.15\linewidth}|>{\centering}p{0.15\linewidth}|>{\centering}p{0.15\linewidth}|>{\centering}p{0.15\linewidth}|>{\centering}p{0.15\linewidth}|}
\hline 
 & AdaptiveSequence<int> & std::deque<int> & std::list<int> & std::vector<int>\tabularnewline
\hline 
push\_back-and-sort: 1,000 elements & 750 milliseconds & 970 milliseconds & 140 milliseconds & 330 milliseconds\tabularnewline
\hline 
\end{tabular}

\caption{\label{tab:Performance-evaluation-results-1}Performance evaluation
results for Iteration 1}
\end{table*}


As can be seen in Table \eqref{tab:Performance-evaluation-results-1},
our \noun{AdaptiveSequence<int>} ran the workload in 77.32\% as much
time as \noun{std::deque<int>}, 435.71\% as much time as \noun{std::list<int>},
and 127.27\% as much time as \noun{std::vector<int>}. The adaptive
sequence beats the worst-case STL sequence, but performs significantly
worse than either of the other choices available.


\subsection{Iteration 2, optimized}

For the second iteration, we evaluated the performance of the various
sequence data structures using three different workloads: a push\_back-push\_front
workload of 1,000,000 elements, a push\_back-and-sort workload of
1,000 elements, and an insertion workload of 100,000 elements. Our
operation-log size, the number of operations to log before checking
whether to adapt, was initially set at 100, and if an adaptation was
not performed after that many operations we doubled it.

\begin{table*}[tbh]
\begin{tabular}{|>{\centering}p{0.15\textwidth}||>{\centering}p{0.2\textwidth}||>{\centering}p{0.2\textwidth}||>{\centering}p{0.2\textwidth}||>{\centering}p{0.15\textwidth}|}
\hline 
\multicolumn{1}{|>{\centering}p{0.15\textwidth}|}{} & \multicolumn{1}{>{\centering}p{0.2\textwidth}|}{AdaptiveSequence<int>} & \multicolumn{1}{>{\centering}p{0.2\textwidth}|}{std::deque<int>} & \multicolumn{1}{>{\centering}p{0.2\textwidth}|}{std::list<int>} & std::vector<int>\tabularnewline
\hline 
push\_back-push\_front: 1,000,000 elements & 370 milliseconds & 110 milliseconds & 430 milliseconds & N/A\tabularnewline
\hline 
\hline 
push\_back-and-sort: 1,000 elements & 340 milliseconds & 960 milliseconds & 150 milliseconds & 340 milliseconds\tabularnewline
\hline 
\hline 
insertions: 100,000 elements & 2,070 milliseconds & 40 milliseconds & 30 milliseconds & 3,420 milliseconds\tabularnewline
\hline 
\end{tabular}

\caption{\label{tab:Performance-evaluation-results}Performance evaluation
results for Iteration 2}
\end{table*}


In this iteration, we saw significant improvement in the performance
of the adaptive sequence; it became competitive with one or more of
the other data structures in every workload. The raw data can be found
in Table \eqref{tab:Performance-evaluation-results-1}.

Our first (and largest) workload resulted in \noun{AdaptiveSequence<int>}
taking 86.05\% the time of \noun{std::list<int>}, while taking 236.36\%
the time of \noun{std::deque<int>}. \noun{std::vector<int>} does not
support \noun{push\_back()} or \noun{push\_front()} operations natively,
making this workload inapplicable to it and leaving \noun{AdaptiveSequence<int>}
as a {}``winner'' over \noun{std::vector<int>} for this case. The
adaptive sequence eventually adapts to an internal deque.

In our second (and smallest) workload, \noun{AdaptiveSequence<int>}
took 35.42\% as much time as \noun{std::deque<int>}, 126.67\% as much
time as \noun{std::list<int>}, and the same amount of time as \noun{std::vector<int>}.
No adaptation from the default internal data structure, an \noun{std::vector<int>},
is recorded, despite the amenability of lists and deques to \noun{push\_back()}
operations over vectors.

In our third (and second-largest) workload, \noun{AdaptiveSequence<int>}
took 5075\% the time of \noun{std::deque<int>}, 6800\% the time of
\noun{std::list<int>}, and 60.53\% as much time as \noun{std::vector<int>}.
An initial adaptation to an internal list was recorded, which seems
incongruous with the failure to match \noun{std::list<int>} in performance.

It would appear that this iteration of our adaptive sequence code
still requires additional speed-ups to compete with the \emph{best}
correctly-selected STL data structure, but manages to usually beat
the worst selection of STL data structure and compete with the second-best
choice.


\subsection{Second benchmark: randomly-generated workloads}

It is questionable whether our previous benchmark workloads truly
and accurately represented the performance and ideal use-case of an
adaptive sequence. Each workload consisted of large blocks of identical
operations performed in a loop, and therefore could always have performed
better when given a best-choice STL data structure. In real workloads
driven by real large-scale programs, the sequence of operations performed
on a data structure is not likely to be identical, and in fact will
probably vary significantly. In that situation, the adaptivity functionality
should aid performance more than it does in the uniform case.

We thus wrote another benchmark workload that tests the adaptive sequence
in this fashion. It generates a number of random sequence operations
and then loops across this sequence, replaying it on each of the four
possible sequences.


\section{Discussion and Conclusions}

At this point, the adaptive sequence consistently manages to beat
the worst-case STL sequence for performance, and often manages performance
close to that of the second-best for a given workload. Given more
optimization, it could achieve performance comparable to the best-choice
data structure, but it should already be usable for environments in
which the best-choice is difficult to determine.


\subsection{Further possible adaptation techniques}

Our current implementation of the adaptive sequence keeps a log of
previously-performed operations and, when it adapts, adapts to whatever
internal data structure would most quickly have performed the past
workload. The space usage for logging the operations is linear in
the number of operations logged, and time usage for logging the operations
and performing the actual adaptation are O(max(\emph{n},\emph{m})),
where \emph{n} is the number of operations logged and \emph{m} is
the number of elements in the adaptive sequence. Our tests have shown
that while this adaptation technique consistently beats the worst-choice
STL data structure, it usually performs identically or slightly worse
than the second-best-choice.

In order to reduce the overhead of deciding when and how to adapt,
and of the actual adaptation, we could reengineer the internals of
the adaptive sequence to decide these matters using probabilistic
reasoning. The adaptive sequence would have counters for each kind
of operation, each initialized to 1, and would count operations the
operations it has performed. Rather than keeping a log of each operation,
we would log the kind of operation by simply incrementing its counter.
Upon logging \emph{n} operations and with \emph{K} kinds of operations,
we would adapt by calculating the probability \emph{P(k)} that a given
operation will be of kind \emph{k} as: 
\[
P(k)=\frac{C(k)}{n+K}
\]
 Using those probabilities, we could reconstruct a summed-up workload
of \emph{n} operations and calculate its costs for each internal data
structure. For an internal data structure \emph{rep}, an operation
kind \emph{k} with probability \emph{P(k)}, and \emph{n} operations,
we would find: 
\[
\underset{k}{\sum}nP(k)cost(k,rep)
\]


Since we would only have to track one probability for each operation
kind and the counter increments would have to be done only once per
kind per operation, the space costs of logging operations would drop
to O(1) while the time requirements would remain constant. Using the
updated probabilities to summarize the log of operations we're not
keeping will also cut down the asymptotic complexity of adapting itself,
since calculating the costs of internal representations will take
only constant time.
\begin{acks}
We would like to thank Professor Emery Berger.\end{acks}
\begin{thebibliography}{References}
\bibitem{Liu2009}L. Liu and S. Rus. perflint: A Context Sensitive
Performance Advisor for C++ Programs. In \emph{Proceedings of the
2009 International Symposium on Code Generation and Optimization},
March 2009.

\bibitem{Jung2011}Changhee Jung, Silvius Rus, Brian P. Railing, Nathan
Clark, and Santosh Pande. Brainy: Effective Selection of Data Structures.
In \emph{Proceedings of the 2011 ACM conference on Programming Language
Design and Implementation}, preprint, retrieved May 2011.

\bibitem{Krone2003}Joan Krone. Multiple implementations for component
based software using Java interfaces. In \emph{Journal of Computing
in Small Colleges}, volume 19, issue 1, pp 30-38, October 2003.

\bibitem{ISO2009}Working Draft, Standard for Programming Language
C++ (version of 2009-06-22), \emph{International Standards Organization},
June 2009.

\bibitem{Bik1993}Aart Bik, Harry Wijshoff. On Automatic Data Structure
Selection and Code Generation for Sparse Computations. In \emph{Lecture
Notes on Computer Science}, pp 57-75, 1993.

\bibitem{Low1976}James Low and Paul Rovner. Techniques for the Automatic
Selection of Data Structures. In \emph{Proceedings of the 3rd ACM
SIGACT-SIGPLAN Symposium on Principles of Programming Languages},
pp 58-67, 1976.

\bibitem{Wozniak2007}Geoff Wozniak, Mark Daley, and Stephen Watt.
Dynamic ADTs: a {}``don't ask, don't tell'' policy for data abstraction.
In \emph{Proceedings of the 2007 International Lisp Conference}, article
26, 12 pages, 2007.\end{thebibliography}

\end{document}
